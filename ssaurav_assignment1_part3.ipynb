{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61ffe9e",
   "metadata": {},
   "source": [
    "# <center>CSE 4/546: Reinforcement Learning</center>\n",
    "## <center>Prof. Alina Vereshchaka</center>\n",
    "### <center>Assignment 1, Part 3</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db09cc",
   "metadata": {},
   "source": [
    "### Stock Trading Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f827591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Stock Trading Environment.\n",
    "\"\"\"DON'T MAKE ANY CHANGES TO THE ENVIRONMENT.\"\"\"\n",
    "\n",
    "\n",
    "class StockTradingEnvironment(gymnasium.Env):\n",
    "    \"\"\"This class implements the Stock Trading environment.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, train=True, number_of_days_to_consider=10):\n",
    "        \"\"\"This method initializes the environment.\n",
    "\n",
    "        :param file_path: - Path of the CSV file containing the historical stock data.\n",
    "        :param train: - Boolean indicating whether the goal is to train or test the performance of the agent.\n",
    "        :param number_of_days_to_consider = Integer representing the number of days the for which the agent\n",
    "                considers the trend in stock price to make a decision.\"\"\"\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.stock_data = pd.read_csv(self.file_path)\n",
    "        self.train = train\n",
    "\n",
    "        # Splitting the data into train and test datasets.\n",
    "        self.training_stock_data = self.stock_data.iloc[:int(0.8 * len(self.stock_data))]\n",
    "        self.testing_stock_data = self.stock_data.iloc[int(0.8 * len(self.stock_data)):].reset_index()\n",
    "\n",
    "        self.observation_space = spaces.Discrete(4)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
    "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
    "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
    "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
    "        # This defines the agent's total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # List to store the total account value over training or evaluation.\n",
    "        self.total_account_value_list = []\n",
    "        # This defines the number of days for which the agent considers the data before taking an action.\n",
    "        self.number_of_days_to_consider = number_of_days_to_consider\n",
    "        # The maximum timesteps the agent will take before the episode ends.\n",
    "        if self.train:\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "        else:\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "        # Initializing the number of steps taken to 0.\n",
    "        self.timestep = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This method resets the environment and returns the observation.\n",
    "\n",
    "        :returns observation: - Integer in the range of 0 to 3 representing the four possible observations that the\n",
    "                                agent can receive. The observation depends upon whether the price increased on average\n",
    "                                in the number of days the agent considers, and whether the agent already has the stock\n",
    "                                or not.\n",
    "\n",
    "         info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
    "\n",
    "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
    "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
    "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
    "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
    "        # This defines the agent's total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # List to store the total account value over training or evaluation.\n",
    "        self.total_account_value_list = []\n",
    "        # Initializing the number of steps taken to 0.\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Getting the observation vector.\n",
    "        if self.train:\n",
    "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
    "            # subtracted from the  length of the training stock data.\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            stock_held = False\n",
    "\n",
    "            # Observation vector that will be passed to the agent.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        else:\n",
    "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
    "            # number of days considered subtracted from the  length of the testing stock data.\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            stock_held = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        if np.array_equal(observation, [True, False]):\n",
    "            observation = 0\n",
    "        if np.array_equal(observation, [True, True]):\n",
    "            observation = 1\n",
    "        if np.array_equal(observation, [False, False]):\n",
    "            observation = 2\n",
    "        if np.array_equal(observation, [False, True]):\n",
    "            observation = 3\n",
    "\n",
    "        info = None\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"This method implements what happens when the agent takes the action to Buy/Sell/Hold.\n",
    "\n",
    "        :param action: - Integer in the range 0 to 2 inclusive.\n",
    "\n",
    "        :returns observation: - Integer in the range of 0 to 3 representing the four possible observations that the\n",
    "                                agent can receive. The observation depends upon whether the price increased on average\n",
    "                                in the number of days the agent considers, and whether the agent already has the stock\n",
    "                                or not.\n",
    "                 reward: - Integer/Float value that's used to measure the performance of the agent.\n",
    "                 terminated: - Boolean describing whether the episode has terminated.\n",
    "                 truncated: - Boolean describing whether a truncation condition outside the scope of the MDP is satisfied.\n",
    "                 info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
    "\n",
    "        # We give the agent a penalty for taking actions such as buying a stock when the agent doesn't have the\n",
    "        # investment capital and selling a stock when the agent doesn't have any shares.\n",
    "        penalty = 0\n",
    "\n",
    "        if self.train:\n",
    "            if action == 0:  # Buy\n",
    "                if self.number_of_shares > 0:\n",
    "                    penalty = -10\n",
    "                # Determining the number of shares the agent can buy.\n",
    "                number_of_shares_to_buy = math.floor(self.investment_capital / self.training_stock_data[\n",
    "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
    "                # Adding to the number of shares the agent has.\n",
    "                self.number_of_shares += number_of_shares_to_buy\n",
    "\n",
    "                # Computing the stock value, book value, investment capital and reward.\n",
    "                if number_of_shares_to_buy > 0:\n",
    "                    self.stock_value +=\\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.book_value += \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider]\\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.investment_capital -= \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "\n",
    "                    reward = 1 + penalty\n",
    "\n",
    "                else:\n",
    "                    # Computing the stock value and reward.\n",
    "                    self.stock_value = \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * self.number_of_shares\n",
    "                    reward = -10\n",
    "\n",
    "            if action == 1:  # Sell\n",
    "                # Computing the investment capital, sell value and reward.\n",
    "                self.investment_capital += \\\n",
    "                    self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                    * self.number_of_shares\n",
    "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                             * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -10\n",
    "\n",
    "                self.number_of_shares = 0\n",
    "                self.stock_value = 0\n",
    "                self.book_value = 0\n",
    "\n",
    "            if action == 2:  # Hold\n",
    "                # Computing the stock value and reward.\n",
    "                self.stock_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                                   * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -1\n",
    "\n",
    "        else:\n",
    "            if action == 0:  # Buy\n",
    "                if self.number_of_shares > 0:\n",
    "                    penalty = -10\n",
    "                # Determining the number of shares the agent can buy.\n",
    "                number_of_shares_to_buy = math.floor(self.investment_capital / self.testing_stock_data[\n",
    "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
    "                # Adding to the number of shares the agent has.\n",
    "                self.number_of_shares += number_of_shares_to_buy\n",
    "\n",
    "                # Computing the stock value, book value, investment capital and reward.\n",
    "                if number_of_shares_to_buy > 0:\n",
    "                    self.stock_value += \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.book_value += \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.investment_capital -= \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "\n",
    "                    reward = 1 + penalty\n",
    "\n",
    "                else:\n",
    "                    # Computing the stock value and reward.\n",
    "                    self.stock_value = self.training_stock_data['Open'][\n",
    "                                           self.timestep + self.number_of_days_to_consider] * self.number_of_shares\n",
    "                    reward = -10\n",
    "\n",
    "            if action == 1:  # Sell\n",
    "                # Computing the investment capital, sell value and reward.\n",
    "                self.investment_capital += \\\n",
    "                    self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                    * self.number_of_shares\n",
    "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                             * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -10\n",
    "\n",
    "                self.number_of_shares = 0\n",
    "                self.stock_value = 0\n",
    "                self.book_value = 0\n",
    "\n",
    "            if action == 2:  # Hold\n",
    "                # Computing the stock value and reward.\n",
    "                self.stock_value = self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                                   * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -1\n",
    "\n",
    "        # Determining if the agent currently has shares of the stock or not.\n",
    "        if self.number_of_shares > 0:\n",
    "            stock_held = True\n",
    "        else:\n",
    "            stock_held = False\n",
    "\n",
    "        # Getting the observation vector.\n",
    "        if self.train:\n",
    "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
    "            # subtracted from the  length of the training stock data.\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        else:\n",
    "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
    "            # number of days considered subtracted from the  length of the testing stock data.\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        self.timestep += 1  # Increasing the number of steps taken by the agent by 1.\n",
    "\n",
    "        if np.array_equal(observation, [True, False]):\n",
    "            observation = 0\n",
    "        if np.array_equal(observation, [True, True]):\n",
    "            observation = 1\n",
    "        if np.array_equal(observation, [False, False]):\n",
    "            observation = 2\n",
    "        if np.array_equal(observation, [False, True]):\n",
    "            observation = 3\n",
    "\n",
    "        # Computing the total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # Appending the total account value of the list to plot the graph.\n",
    "        self.total_account_value_list.append(self.total_account_value)\n",
    "\n",
    "        # The episode terminates when the maximum timesteps have been reached.\n",
    "        terminated = True if (self.timestep >= self.max_timesteps) \\\n",
    "            else False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"This method renders the agent's total account value over time.\n",
    "\n",
    "        :param mode: 'human' renders to the current display or terminal and returns nothing.\"\"\"\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.plot(self.total_account_value_list, color='lightseagreen', linewidth=7)\n",
    "        plt.xlabel('Days', fontsize=32)\n",
    "        plt.ylabel('Total Account Value', fontsize=32)\n",
    "        plt.title('Total Account Value over Time', fontsize=38)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca06c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can adjust the parameter 'number_of_days_to_consider'. To train the agent, set 'train=True', and to evaluate the agent, set 'train=False'.\n",
    "\n",
    "stock_trading_environment = StockTradingEnvironment(file_path='./NVDA.csv', train=True, number_of_days_to_consider=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445893f5bea20c56",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TO DO: Implement the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8c29e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "#   Q-Learning For Stock Market \n",
    "# -----------------------------------------------------\n",
    "\n",
    "def default_q():\n",
    "    \"\"\"\n",
    "    I define a default Q-function for any new state encountered.\n",
    "    The environment has 3 possible actions: 0 (Buy), 1 (Sell), 2 (Hold).\n",
    "    Therefore, I return an array of length 3 initialized to zeros.\n",
    "    \"\"\"\n",
    "    return np.zeros(3)\n",
    "\n",
    "def train_stock_Q(env, hyperparams):\n",
    "    \"\"\"\n",
    "    I train a Q-learning agent on the StockTradingEnvironment.\n",
    "    \n",
    "    :param env: The StockTradingEnvironment instance (train=True).\n",
    "    :param hyperparams: Dictionary of hyperparameters.\n",
    "    \n",
    "    :return: (Q, rewards_per_episode, eps_history)\n",
    "        Q: A defaultdict mapping state -> Q-values (array of length 3).\n",
    "        rewards_per_episode: List of cumulative rewards per episode.\n",
    "        eps_history: List of epsilon values per episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # -- I read the hyperparameters --\n",
    "    alpha = hyperparams.get('alpha', 0.1)  # Learning rate\n",
    "    gamma = hyperparams.get('gamma', 0.95)  # Discount factor\n",
    "    epsilon = hyperparams.get('epsilon', 1.0)  # Initial exploration rate\n",
    "    epsilon_decay = hyperparams.get('epsilon_decay', 0.995)  # Epsilon decay rate\n",
    "    epsilon_min = hyperparams.get('epsilon_min', 0.01)  # Minimum epsilon value\n",
    "    episodes = hyperparams.get('episodes', 200)  # Number of training episodes\n",
    "\n",
    "    # -- I create a Q-table as a defaultdict --\n",
    "    Q = defaultdict(default_q)\n",
    "\n",
    "    # -- I initialize lists to store stats --\n",
    "    rewards_per_episode = []\n",
    "    eps_history = []\n",
    "\n",
    "    # -- I run the Q-learning loop for the specified number of episodes --\n",
    "    for ep in range(episodes):\n",
    "        # I reset the environment to start a new episode\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        # At the start, I'm not done or truncated\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        print(f\"\\n--- Training Episode {ep+1} starting ---\")\n",
    "\n",
    "        step_count = 0\n",
    "\n",
    "        # I continue until the environment signals termination\n",
    "        while not done and not truncated:\n",
    "            # Possible actions in the stock environment are [0, 1, 2] = [Buy, Sell, Hold]\n",
    "            safe_actions = [0, 1, 2]\n",
    "\n",
    "            # I pick an action using epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                # I explore with a random action\n",
    "                action = random.choice(safe_actions)\n",
    "            else:\n",
    "                # I exploit the best known action from the Q-table\n",
    "                q_vals = Q[state]  # Q-values for the current state (array of length 3)\n",
    "                action = np.argmax(q_vals)\n",
    "\n",
    "            # I take the chosen action in the environment\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # I accumulate the reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # I update the Q-value using the Q-learning formula\n",
    "            best_next = np.max(Q[next_state])  # best Q-value for next_state\n",
    "            Q[state][action] += alpha * (reward + gamma * best_next - Q[state][action])\n",
    "\n",
    "            # I move on to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            # I display interim info\n",
    "            step_count += 1\n",
    "            # print(f\"Step {step_count} - Cumulative Reward: {total_reward}\")\n",
    "\n",
    "        # After this episode finishes, I decay epsilon for the next episode\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        # I store stats for later analysis\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        eps_history.append(epsilon)\n",
    "\n",
    "        # (Optional) I can render after each episode, if needed:\n",
    "        # env.render()\n",
    "\n",
    "        # I log episode summary\n",
    "        print(f\"Episode {ep+1}/{episodes} | Steps: {step_count} | \"\n",
    "              f\"Total Reward: {total_reward} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    # I save the Q-table to disk\n",
    "    q_table_filename = \"part_3_files/part_3_q_table.pkl\",\n",
    "    with open(q_table_filename, \"wb\") as pf:\n",
    "        pickle.dump(Q, pf)\n",
    "    print(f\"Q-table saved to {q_table_filename}\")\n",
    "    \n",
    "    return Q, rewards_per_episode, eps_history\n",
    "\n",
    "\n",
    "def evaluate_stock_Q(env, q_table_filename=\"part_3_files/part_3_q_table.pkl\",\n",
    "                                episodes=10):\n",
    "    \"\"\"\n",
    "    I evaluate a trained Q-learning agent on the StockTradingEnvironment in 'test' mode.\n",
    "    \n",
    "    :param env: The StockTradingEnvironment instance (train=False).\n",
    "    :param q_table_filename: Path to the saved Q-table pickle file.\n",
    "    :param episodes: Number of evaluation episodes.\n",
    "    :param max_steps: Max steps per episode (if you want to limit the episode length).\n",
    "    :return: rewards, a list of total rewards per episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # I load the Q-table\n",
    "    with open(q_table_filename, \"rb\") as pf:\n",
    "        Q = pickle.load(pf)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    # I run multiple episodes for evaluation\n",
    "    for ep in range(episodes):\n",
    "        # I reset and get the initial state\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # I continue until the environment signals termination\n",
    "        while not done and not truncated:\n",
    "\n",
    "            # I pick the best known action from the Q-table\n",
    "            # If the state is not in Q yet, or if Q[state] is uninitialized, fall back to random\n",
    "            if state not in Q:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            # Step in the environment\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "\n",
    "        # Episode finished\n",
    "        rewards.append(total_reward)\n",
    "        env.render()\n",
    "        print(f\"Evaluation Episode {ep+1}/{episodes} | Total Reward: {total_reward}\")\n",
    "\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad35938",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment(file_path='./NVDA.csv', train=True, number_of_days_to_consider=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1be54242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Episode 1 starting ---\n",
      "Episode 1/20 | Steps: 408 | Total Reward: -1283.1741635316594 | Epsilon: 0.999\n",
      "\n",
      "--- Training Episode 2 starting ---\n",
      "Episode 2/20 | Steps: 408 | Total Reward: -1305.855114687485 | Epsilon: 0.998\n",
      "\n",
      "--- Training Episode 3 starting ---\n",
      "Episode 3/20 | Steps: 408 | Total Reward: -1147.6421983922683 | Epsilon: 0.997\n",
      "\n",
      "--- Training Episode 4 starting ---\n",
      "Episode 4/20 | Steps: 408 | Total Reward: -1164.0890604309307 | Epsilon: 0.996\n",
      "\n",
      "--- Training Episode 5 starting ---\n",
      "Episode 5/20 | Steps: 408 | Total Reward: -673.9476331080374 | Epsilon: 0.995\n",
      "\n",
      "--- Training Episode 6 starting ---\n",
      "Episode 6/20 | Steps: 408 | Total Reward: -1376.5901457921748 | Epsilon: 0.994\n",
      "\n",
      "--- Training Episode 7 starting ---\n",
      "Episode 7/20 | Steps: 408 | Total Reward: -900.3574909599397 | Epsilon: 0.993\n",
      "\n",
      "--- Training Episode 8 starting ---\n",
      "Episode 8/20 | Steps: 408 | Total Reward: -1266.6588332600663 | Epsilon: 0.992\n",
      "\n",
      "--- Training Episode 9 starting ---\n",
      "Episode 9/20 | Steps: 408 | Total Reward: -1301.724287738561 | Epsilon: 0.991\n",
      "\n",
      "--- Training Episode 10 starting ---\n",
      "Episode 10/20 | Steps: 408 | Total Reward: -1094.9360263297008 | Epsilon: 0.990\n",
      "\n",
      "--- Training Episode 11 starting ---\n",
      "Episode 11/20 | Steps: 408 | Total Reward: -1238.2906758366844 | Epsilon: 0.989\n",
      "\n",
      "--- Training Episode 12 starting ---\n",
      "Episode 12/20 | Steps: 408 | Total Reward: -1134.5974017444355 | Epsilon: 0.988\n",
      "\n",
      "--- Training Episode 13 starting ---\n",
      "Episode 13/20 | Steps: 408 | Total Reward: -1081.5806868055242 | Epsilon: 0.987\n",
      "\n",
      "--- Training Episode 14 starting ---\n",
      "Episode 14/20 | Steps: 408 | Total Reward: -1394.9622690775398 | Epsilon: 0.986\n",
      "\n",
      "--- Training Episode 15 starting ---\n",
      "Episode 15/20 | Steps: 408 | Total Reward: -1255.0856225150603 | Epsilon: 0.985\n",
      "\n",
      "--- Training Episode 16 starting ---\n",
      "Episode 16/20 | Steps: 408 | Total Reward: -1204.470459350689 | Epsilon: 0.984\n",
      "\n",
      "--- Training Episode 17 starting ---\n",
      "Episode 17/20 | Steps: 408 | Total Reward: -1041.9212223083016 | Epsilon: 0.983\n",
      "\n",
      "--- Training Episode 18 starting ---\n",
      "Episode 18/20 | Steps: 408 | Total Reward: -905.5598521348894 | Epsilon: 0.982\n",
      "\n",
      "--- Training Episode 19 starting ---\n",
      "Episode 19/20 | Steps: 408 | Total Reward: -1191.7462068847815 | Epsilon: 0.981\n",
      "\n",
      "--- Training Episode 20 starting ---\n",
      "Episode 20/20 | Steps: 408 | Total Reward: -1193.0290536630098 | Epsilon: 0.980\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m {   \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisodes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m Q, training_rewards, esp_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stock_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_trading_environment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 102\u001b[0m, in \u001b[0;36mtrain_stock_Q\u001b[0;34m(env, hyperparams)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# I save the Q-table to disk\u001b[39;00m\n\u001b[1;32m    101\u001b[0m q_table_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart_3_files/part_3_q_table.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mq_table_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pf:\n\u001b[1;32m    103\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(Q, pf)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ-table saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_table_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not tuple"
     ]
    }
   ],
   "source": [
    "hyperparams = {   \n",
    "    'alpha': 0.1,\n",
    "    'gamma': 0.95,\n",
    "    'epsilon': 1.0,\n",
    "    'epsilon_decay': 0.999,\n",
    "    'epsilon_min': 0.01,\n",
    "    'episodes': 20,\n",
    "}\n",
    "\n",
    "Q, training_rewards, esp_hist = train_stock_Q(stock_trading_environment, hyperparams=hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e05f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment(file_path='./NVDA.csv', train=True, number_of_days_to_consider=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aeca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting total reward per episode\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(training_rewards, color='blue', linewidth=1.5)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Total Reward\", fontsize=14)\n",
    "plt.title(\"Total Reward per Episode\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting epsilon decay over episodes\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(esp_hist, color='red', linewidth=1.5)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Epsilon\", fontsize=14)\n",
    "plt.title(\"Epsilon Decay\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50405d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Printing initial Q-tables, which are empty since I am using a dictionary for easier implementation. \n",
    "I am commenting out this entire block because the Q-values/tables are very large. \n",
    "The trained Q-table has been saved in this folder. \n",
    "\n",
    "To print the saved Q-values/tables, simply uncomment the code below and run it.\n",
    "\"\"\"\n",
    "\n",
    "q_table_filename = \"part_3_files/part_3_q_table.pkl\"\n",
    "\n",
    "print(\"\\nInitial Q-table (empty entries as I am using a dictionary for easier implementation):\")\n",
    "\n",
    "Q = defaultdict(default_q)\n",
    "for k, v in Q.items():\n",
    "    print(k, v)\n",
    "\n",
    "# Loading the Q-table from the pickle file\n",
    "with open(q_table_filename, \"rb\") as pf:\n",
    "    Q = pickle.load(pf)\n",
    "\n",
    "# Printing the final Q-table\n",
    "print(\"\\nTrained Q-table (for visited states):\\n\")\n",
    "for k, v in Q.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f68568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the trained Q-table\n",
    "q_table_filename = \"part_3_files/part_3_q_table.pkl\"\n",
    "\n",
    "prod_stock_trading_environment = StockTradingEnvironment(file_path='./NVDA.csv', train=False, number_of_days_to_consider=10)\n",
    "\n",
    "rewards = evaluate_stock_Q(prod_stock_trading_environment, q_table_filename=q_table_filename, episodes= 10)\n",
    "\n",
    "# Plotting greedy policy evaluation\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(rewards, marker='o', linestyle='-', color='green', markersize=8)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Total Reward\", fontsize=14)\n",
    "plt.title(\"Greedy Policy Evaluation (20 Episodes)\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a03896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
