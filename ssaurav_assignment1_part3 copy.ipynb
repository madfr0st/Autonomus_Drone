{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# <center>CSE 4/546: Reinforcement Learning</center>\n",
    "## <center>Prof. Alina Vereshchaka</center>\n",
    "### <center>Assignment 1, Part 3</center>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:28:04.734979Z",
     "start_time": "2025-02-20T02:28:04.172933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stock Trading Environment "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:28:04.757454Z",
     "start_time": "2025-02-20T02:28:04.744781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the Stock Trading Environment.\n",
    "\"\"\"DON'T MAKE ANY CHANGES TO THE ENVIRONMENT.\"\"\"\n",
    "\n",
    "\n",
    "class StockTradingEnvironment(gymnasium.Env):\n",
    "    \"\"\"This class implements the Stock Trading environment.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, train=True, number_of_days_to_consider=10):\n",
    "        \"\"\"This method initializes the environment.\n",
    "\n",
    "        :param file_path: - Path of the CSV file containing the historical stock data.\n",
    "        :param train: - Boolean indicating whether the goal is to train or test the performance of the agent.\n",
    "        :param number_of_days_to_consider = Integer representing the number of days the for which the agent\n",
    "                considers the trend in stock price to make a decision.\"\"\"\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.stock_data = pd.read_csv(self.file_path)\n",
    "        self.train = train\n",
    "\n",
    "        # Splitting the data into train and test datasets.\n",
    "        self.training_stock_data = self.stock_data.iloc[:int(0.8 * len(self.stock_data))]\n",
    "        self.testing_stock_data = self.stock_data.iloc[int(0.8 * len(self.stock_data)):].reset_index()\n",
    "\n",
    "        self.observation_space = spaces.Discrete(4)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
    "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
    "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
    "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
    "        # This defines the agent's total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # List to store the total account value over training or evaluation.\n",
    "        self.total_account_value_list = []\n",
    "        # This defines the number of days for which the agent considers the data before taking an action.\n",
    "        self.number_of_days_to_consider = number_of_days_to_consider\n",
    "        # The maximum timesteps the agent will take before the episode ends.\n",
    "        if self.train:\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "        else:\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "        # Initializing the number of steps taken to 0.\n",
    "        self.timestep = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This method resets the environment and returns the observation.\n",
    "\n",
    "        :returns observation: - Integer in the range of 0 to 3 representing the four possible observations that the\n",
    "                                agent can receive. The observation depends upon whether the price increased on average\n",
    "                                in the number of days the agent considers, and whether the agent already has the stock\n",
    "                                or not.\n",
    "\n",
    "         info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
    "\n",
    "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
    "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
    "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
    "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
    "        # This defines the agent's total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # List to store the total account value over training or evaluation.\n",
    "        self.total_account_value_list = []\n",
    "        # Initializing the number of steps taken to 0.\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Getting the observation vector.\n",
    "        if self.train:\n",
    "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
    "            # subtracted from the  length of the training stock data.\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            stock_held = False\n",
    "\n",
    "            # Observation vector that will be passed to the agent.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        else:\n",
    "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
    "            # number of days considered subtracted from the  length of the testing stock data.\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            stock_held = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        if np.array_equal(observation, [True, False]):\n",
    "            observation = 0\n",
    "        if np.array_equal(observation, [True, True]):\n",
    "            observation = 1\n",
    "        if np.array_equal(observation, [False, False]):\n",
    "            observation = 2\n",
    "        if np.array_equal(observation, [False, True]):\n",
    "            observation = 3\n",
    "\n",
    "        info = None\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"This method implements what happens when the agent takes the action to Buy/Sell/Hold.\n",
    "\n",
    "        :param action: - Integer in the range 0 to 2 inclusive.\n",
    "\n",
    "        :returns observation: - Integer in the range of 0 to 3 representing the four possible observations that the\n",
    "                                agent can receive. The observation depends upon whether the price increased on average\n",
    "                                in the number of days the agent considers, and whether the agent already has the stock\n",
    "                                or not.\n",
    "                 reward: - Integer/Float value that's used to measure the performance of the agent.\n",
    "                 terminated: - Boolean describing whether the episode has terminated.\n",
    "                 truncated: - Boolean describing whether a truncation condition outside the scope of the MDP is satisfied.\n",
    "                 info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
    "\n",
    "        # We give the agent a penalty for taking actions such as buying a stock when the agent doesn't have the\n",
    "        # investment capital and selling a stock when the agent doesn't have any shares.\n",
    "        penalty = 0\n",
    "\n",
    "        if self.train:\n",
    "            if action == 0:  # Buy\n",
    "                if self.number_of_shares > 0:\n",
    "                    penalty = -10\n",
    "                # Determining the number of shares the agent can buy.\n",
    "                number_of_shares_to_buy = math.floor(self.investment_capital / self.training_stock_data[\n",
    "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
    "                # Adding to the number of shares the agent has.\n",
    "                self.number_of_shares += number_of_shares_to_buy\n",
    "\n",
    "                # Computing the stock value, book value, investment capital and reward.\n",
    "                if number_of_shares_to_buy > 0:\n",
    "                    self.stock_value +=\\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.book_value += \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider]\\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.investment_capital -= \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "\n",
    "                    reward = 1 + penalty\n",
    "\n",
    "                else:\n",
    "                    # Computing the stock value and reward.\n",
    "                    self.stock_value = \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * self.number_of_shares\n",
    "                    reward = -10\n",
    "\n",
    "            if action == 1:  # Sell\n",
    "                # Computing the investment capital, sell value and reward.\n",
    "                self.investment_capital += \\\n",
    "                    self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                    * self.number_of_shares\n",
    "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                             * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -10\n",
    "\n",
    "                self.number_of_shares = 0\n",
    "                self.stock_value = 0\n",
    "                self.book_value = 0\n",
    "\n",
    "            if action == 2:  # Hold\n",
    "                # Computing the stock value and reward.\n",
    "                self.stock_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                                   * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -1\n",
    "\n",
    "        else:\n",
    "            if action == 0:  # Buy\n",
    "                if self.number_of_shares > 0:\n",
    "                    penalty = -10\n",
    "                # Determining the number of shares the agent can buy.\n",
    "                number_of_shares_to_buy = math.floor(self.investment_capital / self.testing_stock_data[\n",
    "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
    "                # Adding to the number of shares the agent has.\n",
    "                self.number_of_shares += number_of_shares_to_buy\n",
    "\n",
    "                # Computing the stock value, book value, investment capital and reward.\n",
    "                if number_of_shares_to_buy > 0:\n",
    "                    self.stock_value += \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.book_value += \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.investment_capital -= \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "\n",
    "                    reward = 1 + penalty\n",
    "\n",
    "                else:\n",
    "                    # Computing the stock value and reward.\n",
    "                    self.stock_value = self.training_stock_data['Open'][\n",
    "                                           self.timestep + self.number_of_days_to_consider] * self.number_of_shares\n",
    "                    reward = -10\n",
    "\n",
    "            if action == 1:  # Sell\n",
    "                # Computing the investment capital, sell value and reward.\n",
    "                self.investment_capital += \\\n",
    "                    self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                    * self.number_of_shares\n",
    "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                             * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -10\n",
    "\n",
    "                self.number_of_shares = 0\n",
    "                self.stock_value = 0\n",
    "                self.book_value = 0\n",
    "\n",
    "            if action == 2:  # Hold\n",
    "                # Computing the stock value and reward.\n",
    "                self.stock_value = self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                                   * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -1\n",
    "\n",
    "        # Determining if the agent currently has shares of the stock or not.\n",
    "        if self.number_of_shares > 0:\n",
    "            stock_held = True\n",
    "        else:\n",
    "            stock_held = False\n",
    "\n",
    "        # Getting the observation vector.\n",
    "        if self.train:\n",
    "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
    "            # subtracted from the  length of the training stock data.\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        else:\n",
    "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
    "            # number of days considered subtracted from the  length of the testing stock data.\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        self.timestep += 1  # Increasing the number of steps taken by the agent by 1.\n",
    "\n",
    "        if np.array_equal(observation, [True, False]):\n",
    "            observation = 0\n",
    "        if np.array_equal(observation, [True, True]):\n",
    "            observation = 1\n",
    "        if np.array_equal(observation, [False, False]):\n",
    "            observation = 2\n",
    "        if np.array_equal(observation, [False, True]):\n",
    "            observation = 3\n",
    "\n",
    "        # Computing the total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # Appending the total account value of the list to plot the graph.\n",
    "        self.total_account_value_list.append(self.total_account_value)\n",
    "\n",
    "        # The episode terminates when the maximum timesteps have been reached.\n",
    "        terminated = True if (self.timestep >= self.max_timesteps) \\\n",
    "            else False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"This method renders the agent's total account value over time.\n",
    "\n",
    "        :param mode: 'human' renders to the current display or terminal and returns nothing.\"\"\"\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.plot(self.total_account_value_list, color='lightseagreen', linewidth=7)\n",
    "        plt.xlabel('Days', fontsize=32)\n",
    "        plt.ylabel('Total Account Value', fontsize=32)\n",
    "        plt.title('Total Account Value over Time', fontsize=38)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:28:04.821907Z",
     "start_time": "2025-02-20T02:28:04.817005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE: You can adjust the parameter 'number_of_days_to_consider'. To train the agent, set 'train=True', and to evaluate the agent, set 'train=False'.\n",
    "\n",
    "stock_trading_environment = StockTradingEnvironment(file_path='./NVDA.csv', train=True, number_of_days_to_consider=10)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TO DO: Implement the Q-learning algorithm"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:28:04.837747Z",
     "start_time": "2025-02-20T02:28:04.832813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------\n",
    "#   Q-Learning For Stock Market \n",
    "# -----------------------------------------------------\n",
    "\n",
    "def default_q():\n",
    "    \"\"\"\n",
    "    I define a default Q-function for any new state encountered.\n",
    "    The environment has 3 possible actions: 0 (Buy), 1 (Sell), 2 (Hold).\n",
    "    Therefore, I return an array of length 3 initialized to zeros.\n",
    "    \"\"\"\n",
    "    return np.zeros(3)\n",
    "\n",
    "def train_stock_Q(env, hyperparams):\n",
    "    \"\"\"\n",
    "    I train a Q-learning agent on the StockTradingEnvironment.\n",
    "    \n",
    "    :param env: The StockTradingEnvironment instance (train=True).\n",
    "    :param hyperparams: Dictionary of hyperparameters.\n",
    "    \n",
    "    :return: (Q, rewards_per_episode, eps_history)\n",
    "        Q: A defaultdict mapping state -> Q-values (array of length 3).\n",
    "        rewards_per_episode: List of cumulative rewards per episode.\n",
    "        eps_history: List of epsilon values per episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # -- I read the hyperparameters --\n",
    "    alpha = hyperparams.get('alpha', 0.1)  # Learning rate\n",
    "    gamma = hyperparams.get('gamma', 0.95)  # Discount factor\n",
    "    epsilon = hyperparams.get('epsilon', 1.0)  # Initial exploration rate\n",
    "    epsilon_decay = hyperparams.get('epsilon_decay', 0.995)  # Epsilon decay rate\n",
    "    epsilon_min = hyperparams.get('epsilon_min', 0.01)  # Minimum epsilon value\n",
    "    episodes = hyperparams.get('episodes', 200)  # Number of training episodes\n",
    "\n",
    "    # -- I create a Q-table as a defaultdict --\n",
    "    Q = defaultdict(default_q)\n",
    "\n",
    "    # -- I initialize lists to store stats --\n",
    "    rewards_per_episode = []\n",
    "    eps_history = []\n",
    "\n",
    "    # -- I run the Q-learning loop for the specified number of episodes --\n",
    "    for ep in range(episodes):\n",
    "        # I reset the environment to start a new episode\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        # At the start, I'm not done or truncated\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        print(f\"\\n--- Training Episode {ep+1} starting ---\")\n",
    "\n",
    "        step_count = 0\n",
    "\n",
    "        # I continue until the environment signals termination\n",
    "        while not done and not truncated:\n",
    "            # Possible actions in the stock environment are [0, 1, 2] = [Buy, Sell, Hold]\n",
    "            safe_actions = [0, 1, 2]\n",
    "\n",
    "            # I pick an action using epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                # I explore with a random action\n",
    "                action = random.choice(safe_actions)\n",
    "            else:\n",
    "                # I exploit the best known action from the Q-table\n",
    "                q_vals = Q[state]  # Q-values for the current state (array of length 3)\n",
    "                action = np.argmax(q_vals)\n",
    "\n",
    "            # I take the chosen action in the environment\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # I accumulate the reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # I update the Q-value using the Q-learning formula\n",
    "            best_next = np.max(Q[next_state])  # best Q-value for next_state\n",
    "            Q[state][action] += alpha * (reward + gamma * best_next - Q[state][action])\n",
    "\n",
    "            # I move on to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            # I display interim info\n",
    "            step_count += 1\n",
    "            # print(f\"Step {step_count} - Cumulative Reward: {total_reward}\")\n",
    "\n",
    "        # After this episode finishes, I decay epsilon for the next episode\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        # I store stats for later analysis\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        eps_history.append(epsilon)\n",
    "\n",
    "        # (Optional) I can render after each episode, if needed:\n",
    "        # env.render()\n",
    "\n",
    "        # I log episode summary\n",
    "        print(f\"Episode {ep+1}/{episodes} | Steps: {step_count} | \"\n",
    "              f\"Total Reward: {total_reward} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    # I save the Q-table to disk\n",
    "    q_table_filename = \"part_3_files/part_3_q_table.pkl\"\n",
    "    with open(q_table_filename, \"wb\") as pf:\n",
    "        pickle.dump(Q, pf)\n",
    "    print(f\"Q-table saved to {q_table_filename}\")\n",
    "    \n",
    "    return Q, rewards_per_episode, eps_history\n",
    "\n",
    "\n",
    "def evaluate_stock_Q(env, q_table_filename=\"part_3_files/part_3_q_table.pkl\",\n",
    "                                episodes=10):\n",
    "    \"\"\"\n",
    "    I evaluate a trained Q-learning agent on the StockTradingEnvironment in 'test' mode.\n",
    "    \n",
    "    :param env: The StockTradingEnvironment instance (train=False).\n",
    "    :param q_table_filename: Path to the saved Q-table pickle file.\n",
    "    :param episodes: Number of evaluation episodes.\n",
    "    :param max_steps: Max steps per episode (if you want to limit the episode length).\n",
    "    :return: rewards, a list of total rewards per episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # I load the Q-table\n",
    "    with open(q_table_filename, \"rb\") as pf:\n",
    "        Q = pickle.load(pf)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    # I run multiple episodes for evaluation\n",
    "    for ep in range(episodes):\n",
    "        # I reset and get the initial state\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # I continue until the environment signals termination\n",
    "        while not done and not truncated:\n",
    "\n",
    "            # I pick the best known action from the Q-table\n",
    "         \n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "            # Step in the environment\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "\n",
    "        # Episode finished\n",
    "        rewards.append(total_reward)\n",
    "        env.render()\n",
    "        print(f\"Evaluation Episode {ep+1}/{episodes} | Total Reward: {total_reward}\")\n",
    "\n",
    "    return rewards\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:28:04.886240Z",
     "start_time": "2025-02-20T02:28:04.882631Z"
    }
   },
   "cell_type": "code",
   "source": "stock_trading_environment = StockTradingEnvironment(file_path='./NVDA.csv', train=True, number_of_days_to_consider=10)",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-20T02:28:04.902971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyperparams = {   \n",
    "    'alpha': 0.005,\n",
    "    'gamma': 0.95,\n",
    "    'epsilon': 1.0,\n",
    "    'epsilon_decay': 0.9995,\n",
    "    'epsilon_min': 0.01,\n",
    "    'episodes': 10000,\n",
    "}\n",
    "\n",
    "Q, rewards_per_episode, eps_history = train_stock_Q(stock_trading_environment, hyperparams=hyperparams)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Episode 1 starting ---\n",
      "Episode 1/10000 | Steps: 408 | Total Reward: -1330.3341562391495 | Epsilon: 1.000\n",
      "\n",
      "--- Training Episode 2 starting ---\n",
      "Episode 2/10000 | Steps: 408 | Total Reward: -987.1817049615868 | Epsilon: 0.999\n",
      "\n",
      "--- Training Episode 3 starting ---\n",
      "Episode 3/10000 | Steps: 408 | Total Reward: -1158.578541131356 | Epsilon: 0.999\n",
      "\n",
      "--- Training Episode 4 starting ---\n",
      "Episode 4/10000 | Steps: 408 | Total Reward: -1080.474865589432 | Epsilon: 0.998\n",
      "\n",
      "--- Training Episode 5 starting ---\n",
      "Episode 5/10000 | Steps: 408 | Total Reward: -1227.3025199272972 | Epsilon: 0.998\n",
      "\n",
      "--- Training Episode 6 starting ---\n",
      "Episode 6/10000 | Steps: 408 | Total Reward: -1316.8875092386127 | Epsilon: 0.997\n",
      "\n",
      "--- Training Episode 7 starting ---\n",
      "Episode 7/10000 | Steps: 408 | Total Reward: -1175.9743869330705 | Epsilon: 0.997\n",
      "\n",
      "--- Training Episode 8 starting ---\n",
      "Episode 8/10000 | Steps: 408 | Total Reward: -1219.9091128937932 | Epsilon: 0.996\n",
      "\n",
      "--- Training Episode 9 starting ---\n",
      "Episode 9/10000 | Steps: 408 | Total Reward: -1147.0851961918381 | Epsilon: 0.996\n",
      "\n",
      "--- Training Episode 10 starting ---\n",
      "Episode 10/10000 | Steps: 408 | Total Reward: -1018.3713254187273 | Epsilon: 0.995\n",
      "\n",
      "--- Training Episode 11 starting ---\n",
      "Episode 11/10000 | Steps: 408 | Total Reward: -1222.5498715552744 | Epsilon: 0.995\n",
      "\n",
      "--- Training Episode 12 starting ---\n",
      "Episode 12/10000 | Steps: 408 | Total Reward: -1152.3050397052207 | Epsilon: 0.994\n",
      "\n",
      "--- Training Episode 13 starting ---\n",
      "Episode 13/10000 | Steps: 408 | Total Reward: -1138.941058644262 | Epsilon: 0.994\n",
      "\n",
      "--- Training Episode 14 starting ---\n",
      "Episode 14/10000 | Steps: 408 | Total Reward: -1327.834678734744 | Epsilon: 0.993\n",
      "\n",
      "--- Training Episode 15 starting ---\n",
      "Episode 15/10000 | Steps: 408 | Total Reward: -972.9642510188776 | Epsilon: 0.993\n",
      "\n",
      "--- Training Episode 16 starting ---\n",
      "Episode 16/10000 | Steps: 408 | Total Reward: -1181.6617904613706 | Epsilon: 0.992\n",
      "\n",
      "--- Training Episode 17 starting ---\n",
      "Episode 17/10000 | Steps: 408 | Total Reward: -1239.558814132228 | Epsilon: 0.992\n",
      "\n",
      "--- Training Episode 18 starting ---\n",
      "Episode 18/10000 | Steps: 408 | Total Reward: -1313.08386688697 | Epsilon: 0.991\n",
      "\n",
      "--- Training Episode 19 starting ---\n",
      "Episode 19/10000 | Steps: 408 | Total Reward: -1143.2662450778014 | Epsilon: 0.991\n",
      "\n",
      "--- Training Episode 20 starting ---\n",
      "Episode 20/10000 | Steps: 408 | Total Reward: -1061.7317120477855 | Epsilon: 0.990\n",
      "\n",
      "--- Training Episode 21 starting ---\n",
      "Episode 21/10000 | Steps: 408 | Total Reward: -1065.549285057769 | Epsilon: 0.990\n",
      "\n",
      "--- Training Episode 22 starting ---\n",
      "Episode 22/10000 | Steps: 408 | Total Reward: -993.9047430232328 | Epsilon: 0.989\n",
      "\n",
      "--- Training Episode 23 starting ---\n",
      "Episode 23/10000 | Steps: 408 | Total Reward: -964.7919350503702 | Epsilon: 0.989\n",
      "\n",
      "--- Training Episode 24 starting ---\n",
      "Episode 24/10000 | Steps: 408 | Total Reward: -1278.372469313431 | Epsilon: 0.988\n",
      "\n",
      "--- Training Episode 25 starting ---\n",
      "Episode 25/10000 | Steps: 408 | Total Reward: -1221.338900504947 | Epsilon: 0.988\n",
      "\n",
      "--- Training Episode 26 starting ---\n",
      "Episode 26/10000 | Steps: 408 | Total Reward: -1116.02522080316 | Epsilon: 0.987\n",
      "\n",
      "--- Training Episode 27 starting ---\n",
      "Episode 27/10000 | Steps: 408 | Total Reward: -1255.1558468226153 | Epsilon: 0.987\n",
      "\n",
      "--- Training Episode 28 starting ---\n",
      "Episode 28/10000 | Steps: 408 | Total Reward: -1204.3083691149598 | Epsilon: 0.986\n",
      "\n",
      "--- Training Episode 29 starting ---\n",
      "Episode 29/10000 | Steps: 408 | Total Reward: -1067.6480477775742 | Epsilon: 0.986\n",
      "\n",
      "--- Training Episode 30 starting ---\n",
      "Episode 30/10000 | Steps: 408 | Total Reward: -1069.5750992958879 | Epsilon: 0.985\n",
      "\n",
      "--- Training Episode 31 starting ---\n",
      "Episode 31/10000 | Steps: 408 | Total Reward: -1220.3932262800447 | Epsilon: 0.985\n",
      "\n",
      "--- Training Episode 32 starting ---\n",
      "Episode 32/10000 | Steps: 408 | Total Reward: -1180.4315812011146 | Epsilon: 0.984\n",
      "\n",
      "--- Training Episode 33 starting ---\n",
      "Episode 33/10000 | Steps: 408 | Total Reward: -1207.6708500047034 | Epsilon: 0.984\n",
      "\n",
      "--- Training Episode 34 starting ---\n",
      "Episode 34/10000 | Steps: 408 | Total Reward: -1398.307962310921 | Epsilon: 0.983\n",
      "\n",
      "--- Training Episode 35 starting ---\n",
      "Episode 35/10000 | Steps: 408 | Total Reward: -1370.2840081330785 | Epsilon: 0.983\n",
      "\n",
      "--- Training Episode 36 starting ---\n",
      "Episode 36/10000 | Steps: 408 | Total Reward: -1151.6064552933692 | Epsilon: 0.982\n",
      "\n",
      "--- Training Episode 37 starting ---\n",
      "Episode 37/10000 | Steps: 408 | Total Reward: -1184.9197173873417 | Epsilon: 0.982\n",
      "\n",
      "--- Training Episode 38 starting ---\n",
      "Episode 38/10000 | Steps: 408 | Total Reward: -744.6248394459496 | Epsilon: 0.981\n",
      "\n",
      "--- Training Episode 39 starting ---\n",
      "Episode 39/10000 | Steps: 408 | Total Reward: -1283.9224159133407 | Epsilon: 0.981\n",
      "\n",
      "--- Training Episode 40 starting ---\n",
      "Episode 40/10000 | Steps: 408 | Total Reward: -1188.2744529574497 | Epsilon: 0.980\n",
      "\n",
      "--- Training Episode 41 starting ---\n",
      "Episode 41/10000 | Steps: 408 | Total Reward: -1372.9537962727381 | Epsilon: 0.980\n",
      "\n",
      "--- Training Episode 42 starting ---\n",
      "Episode 42/10000 | Steps: 408 | Total Reward: -1382.7747612828043 | Epsilon: 0.979\n",
      "\n",
      "--- Training Episode 43 starting ---\n",
      "Episode 43/10000 | Steps: 408 | Total Reward: -1176.490574171655 | Epsilon: 0.979\n",
      "\n",
      "--- Training Episode 44 starting ---\n",
      "Episode 44/10000 | Steps: 408 | Total Reward: -1290.8928806791882 | Epsilon: 0.978\n",
      "\n",
      "--- Training Episode 45 starting ---\n",
      "Episode 45/10000 | Steps: 408 | Total Reward: -1159.5796237431377 | Epsilon: 0.978\n",
      "\n",
      "--- Training Episode 46 starting ---\n",
      "Episode 46/10000 | Steps: 408 | Total Reward: -873.1595472601305 | Epsilon: 0.977\n",
      "\n",
      "--- Training Episode 47 starting ---\n",
      "Episode 47/10000 | Steps: 408 | Total Reward: -1152.5197078679748 | Epsilon: 0.977\n",
      "\n",
      "--- Training Episode 48 starting ---\n",
      "Episode 48/10000 | Steps: 408 | Total Reward: -1019.2278929153497 | Epsilon: 0.976\n",
      "\n",
      "--- Training Episode 49 starting ---\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "stock_trading_environment = StockTradingEnvironment(file_path='./NVDA.csv', train=True, number_of_days_to_consider=10)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting total reward per episode\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(rewards_per_episode, color='blue', linewidth=1.5)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Total Reward\", fontsize=14)\n",
    "plt.title(\"Total Reward per Episode\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting epsilon decay over episodes\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(eps_history, color='red', linewidth=1.5)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Epsilon\", fontsize=14)\n",
    "plt.title(\"Epsilon Decay\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Printing initial Q-tables, which are empty since I am using a dictionary for easier implementation. \n",
    "I am commenting out this entire block because the Q-values/tables are very large. \n",
    "The trained Q-table has been saved in this folder. \n",
    "\n",
    "To print the saved Q-values/tables, simply uncomment the code below and run it.\n",
    "\"\"\"\n",
    "\n",
    "q_table_filename = \"part_3_files/part_3_q_table.pkl\"\n",
    "\n",
    "print(\"\\nInitial Q-table (empty entries as I am using a dictionary for easier implementation):\")\n",
    "\n",
    "Q = defaultdict(default_q)\n",
    "for k, v in Q.items():\n",
    "    print(k, v)\n",
    "\n",
    "# Loading the Q-table from the pickle file\n",
    "with open(q_table_filename, \"rb\") as pf:\n",
    "    Q = pickle.load(pf)\n",
    "\n",
    "# Printing the final Q-table\n",
    "print(\"\\nTrained Q-table (for visited states):\\n\")\n",
    "for k, v in Q.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loading the trained Q-table\n",
    "q_table_filename = \"part_3_files/part_3_q_table.pkl\"\n",
    "\n",
    "prod_stock_trading_environment = StockTradingEnvironment(file_path='./NVDA.csv', train=False, number_of_days_to_consider=10)\n",
    "\n",
    "rewards = evaluate_stock_Q(prod_stock_trading_environment, q_table_filename=q_table_filename, episodes= 1)\n",
    "\n",
    "# Plotting greedy policy evaluation\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(rewards, marker='o', linestyle='-', color='green', markersize=8)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Total Reward\", fontsize=14)\n",
    "plt.title(\"Greedy Policy Evaluation (20 Episodes)\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
